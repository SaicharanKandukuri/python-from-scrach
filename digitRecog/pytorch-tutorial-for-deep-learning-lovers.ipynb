{"metadata":{"language_info":{"name":"python","version":"3.9.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3.9.5 64-bit"},"interpreter":{"hash":"25e52016df3e7687197910b605a54609a04c85b3975b8dd53f41e3927f3482cb"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","execution_count":56,"source":["# This Python 3 environment comes with many helpful analytics libraries installed\r\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\r\n","# For example, here's several helpful packages to load in \r\n","\r\n","import numpy as np # linear algebra\r\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n","import matplotlib.pyplot as plt\r\n","# Input data files are available in the \"../input/\" directory.\r\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"],"outputs":[],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-21T18:07:40.835557Z","iopub.execute_input":"2021-08-21T18:07:40.835887Z","iopub.status.idle":"2021-08-21T18:07:40.859967Z","shell.execute_reply.started":"2021-08-21T18:07:40.835815Z","shell.execute_reply":"2021-08-21T18:07:40.859010Z"},"trusted":true}},{"cell_type":"code","execution_count":57,"source":["# import numpy library\r\n","import numpy as np\r\n","\r\n","# numpy array\r\n","array = [[1,2,3],[4,5,6]]\r\n","first_array = np.array(array) # 2x3 array\r\n","print(\"Array Type: {}\".format(type(first_array))) # type\r\n","print(\"Array Shape: {}\".format(np.shape(first_array))) # shape\r\n","print(first_array)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Array Type: <class 'numpy.ndarray'>\n","Array Shape: (2, 3)\n","[[1 2 3]\n"," [4 5 6]]\n"]}],"metadata":{"_uuid":"d60e9b9f706124117b1d7ffcdeefb45b9aca45e6","_cell_guid":"0a70fde1-b9c4-47c5-aed7-1c863b2fd1e1","execution":{"iopub.status.busy":"2021-08-21T18:07:40.861495Z","iopub.execute_input":"2021-08-21T18:07:40.862082Z","iopub.status.idle":"2021-08-21T18:07:40.870851Z","shell.execute_reply.started":"2021-08-21T18:07:40.862024Z","shell.execute_reply":"2021-08-21T18:07:40.869624Z"},"trusted":true}},{"cell_type":"markdown","source":["- We looked at numpy array.\n","- Now examine how we implement tensor(pytorch array)\n","- import pytorch library with import torch\n","- We create tensor with torch.Tensor() method\n","- type: type of the array. In this example it is tensor\n","- shape: shape of the array. Row x Column"],"metadata":{"_uuid":"28a4eb1e95add272552310eacf20740b895e43cd","_cell_guid":"042b8e6f-0be6-43a1-96b4-49b32999a208"}},{"cell_type":"code","execution_count":58,"source":["# import pytorch library\r\n","import torch\r\n","\r\n","# pytorch array\r\n","tensor = torch.Tensor(array)\r\n","print(\"Array Type: {}\".format(tensor.type)) # type\r\n","print(\"Array Shape: {}\".format(tensor.shape)) # shape\r\n","print(tensor)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Array Type: <built-in method type of Tensor object at 0x000001B4C3640E00>\n","Array Shape: torch.Size([2, 3])\n","tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n"]}],"metadata":{"_uuid":"126ea635dff4e2a6bc9a828dc560863e3be6aa74","_cell_guid":"b383b085-a18f-4c18-a093-428336b6acf6","execution":{"iopub.status.busy":"2021-08-21T18:07:40.872856Z","iopub.execute_input":"2021-08-21T18:07:40.873759Z","iopub.status.idle":"2021-08-21T18:07:41.926798Z","shell.execute_reply.started":"2021-08-21T18:07:40.873703Z","shell.execute_reply":"2021-08-21T18:07:41.925887Z"},"trusted":true}},{"cell_type":"markdown","source":["- Allocation is one of the most used technique in coding. Therefore lets learn how to make it with pytorch.\n","- In order to learn, compare numpy and tensor\n","    - np.ones() = torch.ones()\n","    - np.random.rand() = torch.rand()"],"metadata":{"_uuid":"fa1375fffab5b879eb828de7261e3e436ec15102","_cell_guid":"e1f7ac29-8aa8-46ff-929f-94f2cedf7541"}},{"cell_type":"code","execution_count":59,"source":["# numpy ones\r\n","print(\"Numpy {}\\n\".format(np.ones((2,3))))\r\n","\r\n","# pytorch ones\r\n","print(torch.ones((2,3)))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Numpy [[1. 1. 1.]\n"," [1. 1. 1.]]\n","\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n"]}],"metadata":{"_uuid":"2d36d68f3b57eef9d8f0ed94885f3376de92b414","_cell_guid":"741468a5-5d91-48d7-95b0-6d02180d0c09","execution":{"iopub.status.busy":"2021-08-21T18:07:41.929963Z","iopub.execute_input":"2021-08-21T18:07:41.930354Z","iopub.status.idle":"2021-08-21T18:07:41.940463Z","shell.execute_reply.started":"2021-08-21T18:07:41.930171Z","shell.execute_reply":"2021-08-21T18:07:41.939683Z"},"trusted":true}},{"cell_type":"code","execution_count":60,"source":["# numpy random\r\n","print(\"Numpy {}\\n\".format(np.random.rand(2,3)))\r\n","\r\n","# pytorch random\r\n","print(torch.rand(2,3))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Numpy [[0.06383086 0.42288262 0.84468708]\n"," [0.32943528 0.54361772 0.90727758]]\n","\n","tensor([[0.7710, 0.6954, 0.9060],\n","        [0.1814, 0.5483, 0.8047]])\n"]}],"metadata":{"_uuid":"1e6b8ce52af8a26ffc39fcd751a834ea7c870a2d","_cell_guid":"a578ff9f-df45-4acd-b5ec-2e26b2690adb","execution":{"iopub.status.busy":"2021-08-21T18:07:41.944080Z","iopub.execute_input":"2021-08-21T18:07:41.944334Z","iopub.status.idle":"2021-08-21T18:07:41.955313Z","shell.execute_reply.started":"2021-08-21T18:07:41.944287Z","shell.execute_reply":"2021-08-21T18:07:41.954372Z"},"trusted":true}},{"cell_type":"markdown","source":["- Even if when I use pytorch for neural networks, I feel better if I use numpy. Therefore, usually convert result of neural network that is tensor to numpy array to visualize or examine.\n","- Lets look at conversion between tensor and numpy arrays.\n","    - torch.from_numpy(): from numpy to tensor\n","    - numpy(): from tensor to numpy"],"metadata":{"_uuid":"22b5e44de713f58261bf1ff0b3a52b22a81ef1ef","_cell_guid":"b5177215-45b5-40c1-b838-2d0e3acb48ba"}},{"cell_type":"code","execution_count":61,"source":["# random numpy array\r\n","array = np.random.rand(2,2)\r\n","print(\"{} {}\\n\".format(type(array),array))\r\n","\r\n","# from numpy to tensor\r\n","from_numpy_to_tensor = torch.from_numpy(array)\r\n","print(\"{}\\n\".format(from_numpy_to_tensor))\r\n","\r\n","# from tensor to numpy\r\n","tensor = from_numpy_to_tensor\r\n","from_tensor_to_numpy = tensor.numpy()\r\n","print(\"{} {}\\n\".format(type(from_tensor_to_numpy),from_tensor_to_numpy))"],"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'> [[0.83965228 0.54532999]\n"," [0.94879584 0.03722298]]\n","\n","tensor([[0.8397, 0.5453],\n","        [0.9488, 0.0372]], dtype=torch.float64)\n","\n","<class 'numpy.ndarray'> [[0.83965228 0.54532999]\n"," [0.94879584 0.03722298]]\n","\n"]}],"metadata":{"_uuid":"c6d3a7b8e0e42fcadecb16264b0563f74d01439a","_cell_guid":"f2cedc86-bd28-4709-906f-e236f4a4dbbe","execution":{"iopub.status.busy":"2021-08-21T18:07:41.956666Z","iopub.execute_input":"2021-08-21T18:07:41.956954Z","iopub.status.idle":"2021-08-21T18:07:41.969373Z","shell.execute_reply.started":"2021-08-21T18:07:41.956902Z","shell.execute_reply":"2021-08-21T18:07:41.968557Z"},"trusted":true}},{"cell_type":"code","execution_count":62,"source":["# create tensor \r\n","tensor = torch.ones(3,3)\r\n","print(\"\\n\",tensor)\r\n","\r\n","# Resize\r\n","print(\"{}{}\\n\".format(tensor.view(9).shape,tensor.view(9)))\r\n","\r\n","# Addition\r\n","print(\"Addition: {}\\n\".format(torch.add(tensor,tensor)))\r\n","\r\n","# Subtraction\r\n","print(\"Subtraction: {}\\n\".format(tensor.sub(tensor)))\r\n","\r\n","# Element wise multiplication\r\n","print(\"Element wise multiplication: {}\\n\".format(torch.mul(tensor,tensor)))\r\n","\r\n","# Element wise division\r\n","print(\"Element wise division: {}\\n\".format(torch.div(tensor,tensor)))\r\n","\r\n","# Mean\r\n","tensor = torch.Tensor([1,2,3,4,5])\r\n","print(\"Mean: {}\".format(tensor.mean()))\r\n","\r\n","# Standart deviation (std)\r\n","print(\"std: {}\".format(tensor.std()))"],"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","torch.Size([9])tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n","\n","Addition: tensor([[2., 2., 2.],\n","        [2., 2., 2.],\n","        [2., 2., 2.]])\n","\n","Subtraction: tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]])\n","\n","Element wise multiplication: tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","\n","Element wise division: tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","\n","Mean: 3.0\n","std: 1.5811388492584229\n"]}],"metadata":{"_uuid":"66193cb3c790d13b8328c1c1262e1e3c17230bb8","_cell_guid":"e43af8e7-53ab-40bc-a4f8-4cea941c6df0","execution":{"iopub.status.busy":"2021-08-21T18:07:41.972495Z","iopub.execute_input":"2021-08-21T18:07:41.972725Z","iopub.status.idle":"2021-08-21T18:07:41.993132Z","shell.execute_reply.started":"2021-08-21T18:07:41.972679Z","shell.execute_reply":"2021-08-21T18:07:41.992495Z"},"trusted":true}},{"cell_type":"markdown","source":["### Variables\n","- It accumulates gradients. \n","- We will use pytorch in neural network. And as you know, in neural network we have backpropagation where gradients are calculated. Therefore we need to handle gradients. If you do not know neural network, check my deep learning tutorial first because I will not explain detailed the concepts like optimization, loss function or backpropagation. \n","- Deep learning tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n","- Difference between variables and tensor is variable accumulates gradients.\n","- We can make math operations with variables, too.\n","- In order to make backward propagation we need variables"],"metadata":{"_uuid":"ff85694eebe8d02701e20d7c15b0ad2974175dd3","_cell_guid":"9fb8b7d4-848a-4d2c-8436-162fd47a0e11"}},{"cell_type":"code","execution_count":63,"source":["# import variable from pytorch library\r\n","from torch.autograd import Variable\r\n","\r\n","# define variable\r\n","var = Variable(torch.ones(3), requires_grad = True)\r\n","var"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 1., 1.], requires_grad=True)"]},"metadata":{},"execution_count":63}],"metadata":{"_uuid":"83e3222b53be71e5fc7207da552ce9e9b90486dd","_cell_guid":"fd8ceaa3-f1e2-4761-924e-00a6daca4a82","execution":{"iopub.status.busy":"2021-08-21T18:07:41.996324Z","iopub.execute_input":"2021-08-21T18:07:41.996556Z","iopub.status.idle":"2021-08-21T18:07:42.009186Z","shell.execute_reply.started":"2021-08-21T18:07:41.996509Z","shell.execute_reply":"2021-08-21T18:07:42.008484Z"},"trusted":true}},{"cell_type":"markdown","source":["- Assume we have equation y = x^2\n","- Define x = [2,4] variable\n","- After calculation we find that y = [4,16] (y = x^2)\n","- Recap o equation is that o = (1/2)*sum(y) = (1/2)*sum(x^2)\n","- deriavative of o = x\n","- Result is equal to x so gradients are [2,4]\n","- Lets implement"],"metadata":{"_uuid":"1cc3de04f98fc14624a18cceb5a84034b1dc29c7","_cell_guid":"f5d54144-0753-4e2a-bac1-ccfff3084f6e"}},{"cell_type":"code","execution_count":64,"source":["# lets make basic backward propagation\r\n","# we have an equation that is y = x^2\r\n","array = [2,4]\r\n","tensor = torch.Tensor(array)\r\n","x = Variable(tensor, requires_grad = True)\r\n","y = x**2\r\n","print(\" y =  \",y)\r\n","\r\n","# recap o equation o = 1/2*sum(y)\r\n","o = (1/2)*sum(y)\r\n","print(\" o =  \",o)\r\n","\r\n","# backward\r\n","o.backward() # calculates gradients\r\n","\r\n","# As I defined, variables accumulates gradients. In this part there is only one variable x.\r\n","# Therefore variable x should be have gradients\r\n","# Lets look at gradients with x.grad\r\n","print(\"gradients: \",x.grad)"],"outputs":[{"output_type":"stream","name":"stdout","text":[" y =   tensor([ 4., 16.], grad_fn=<PowBackward0>)\n"," o =   tensor(10., grad_fn=<MulBackward0>)\n","gradients:  tensor([2., 4.])\n"]}],"metadata":{"_uuid":"ff4010e2958a72ce45e43118790f3e20dc2abad6","_cell_guid":"cd73c1cf-d250-48e4-bfb7-ffbe8c03c267","execution":{"iopub.status.busy":"2021-08-21T18:07:42.012653Z","iopub.execute_input":"2021-08-21T18:07:42.012884Z","iopub.status.idle":"2021-08-21T18:07:42.081055Z","shell.execute_reply.started":"2021-08-21T18:07:42.012840Z","shell.execute_reply":"2021-08-21T18:07:42.080066Z"},"trusted":true}},{"cell_type":"code","execution_count":65,"source":["# Import Libraries\r\n","import torch\r\n","import torch.nn as nn\r\n","from torch.autograd import Variable\r\n","from torch.utils.data import DataLoader\r\n","import pandas as pd\r\n","from sklearn.model_selection import train_test_split"],"outputs":[],"metadata":{"_uuid":"1382c63fe24710d3b2840e7dcf172cddbf533743","_cell_guid":"a0bf0fa7-c527-4fd3-b504-02a88fc94798","execution":{"iopub.status.busy":"2021-08-21T18:07:43.436976Z","iopub.execute_input":"2021-08-21T18:07:43.437315Z","iopub.status.idle":"2021-08-21T18:07:44.231687Z","shell.execute_reply.started":"2021-08-21T18:07:43.437262Z","shell.execute_reply":"2021-08-21T18:07:44.230491Z"},"trusted":true}},{"cell_type":"code","execution_count":66,"source":["\r\n","# Prepare Dataset\r\n","# load data\r\n","train = pd.read_csv(r\".\\\\train.csv\",dtype = np.float32)\r\n","\r\n","# split data into features(pixels) and labels(numbers from 0 to 9)\r\n","targets_numpy = train.label.values\r\n","features_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\r\n","\r\n","# train test split. Size of train data is 80% and size of test data is 20%. \r\n","features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\r\n","                                                                             targets_numpy,\r\n","                                                                             test_size = 0.2,\r\n","                                                                             random_state = 42) \r\n","\r\n","# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\r\n","featuresTrain = torch.from_numpy(features_train)\r\n","targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\r\n","\r\n","# create feature and targets tensor for test set.\r\n","featuresTest = torch.from_numpy(features_test)\r\n","targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\r\n","\r\n","# batch_size, epoch and iteration\r\n","batch_size = 100\r\n","n_iters = 10000\r\n","num_epochs = n_iters / (len(features_train) / batch_size)\r\n","num_epochs = int(num_epochs)\r\n","\r\n","# Pytorch train and test sets\r\n","train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\r\n","test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\r\n","\r\n","# data loader\r\n","train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\r\n","test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\r\n","\r\n","# visualize one of the images in data set\r\n","plt.imshow(features_numpy[10].reshape(28,28))\r\n","plt.axis(\"off\")\r\n","plt.title(str(targets_numpy[10]))\r\n","plt.savefig('graph.png')\r\n","plt.show()"],"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"246.958125pt\" version=\"1.1\" viewBox=\"0 0 231.84 246.958125\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-21T23:55:51.826658</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 246.958125 \r\nL 231.84 246.958125 \r\nL 231.84 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g clip-path=\"url(#p139f29a81e)\">\r\n    <image height=\"218\" id=\"image190a351c46\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-21.758125\"/>\r\n   </g>\r\n   <g id=\"text_1\">\r\n    <!-- 8.0 -->\r\n    <g transform=\"translate(106.378125 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-38\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n     <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p139f29a81e\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n","image/png":"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"},"metadata":{"needs_background":"light"}}],"metadata":{"_uuid":"c6e0d7d3843719091564a580dbe08f67ee0d93ec","_cell_guid":"59cdc9d5-da8f-4d7a-abc5-c62b0008afb0","execution":{"iopub.status.busy":"2021-08-21T18:07:44.233726Z","iopub.execute_input":"2021-08-21T18:07:44.234059Z","iopub.status.idle":"2021-08-21T18:07:51.300948Z","shell.execute_reply.started":"2021-08-21T18:07:44.234000Z","shell.execute_reply":"2021-08-21T18:07:51.299890Z"},"trusted":true}},{"cell_type":"markdown","source":["<a id=\"4\"></a> <br>\n","### Artificial Neural Network (ANN)\n","- Logistic regression is good at classification but when complexity(non linearity) increases, the accuracy of model decreases.\n","- Therefore, we need to increase complexity of model.\n","- In order to increase complexity of model, we need to add more non linear functions as hidden layer. \n","- I am saying again that if you do not know what is artificial neural network check my deep learning tutorial because I will not explain neural network detailed here, only explain pytorch.\n","- Artificial Neural Network tutorial: https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n","- What we expect from artificial neural network is that when complexity increases, we use more hidden layers and our model can adapt better. As a result accuracy increase.\n","- **Steps of ANN:**\n","    1. Import Libraries\n","        - In order to show you, I import again but we actually imported them at previous parts.\n","    1. Prepare Dataset\n","        - Totally same with previous part(logistic regression).\n","        - We use same dataset so we only need train_loader and test_loader. \n","        - We use same batch size, epoch and iteration numbers.\n","    1. Create ANN Model\n","        - We add 3 hidden layers.\n","        - We use ReLU, Tanh and ELU activation functions for diversity.\n","    1. Instantiate Model Class\n","        - input_dim = 28*28 # size of image px*px\n","        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n","        - Hidden layer dimension is 150. I only choose it as 150 there is no reason. Actually hidden layer dimension is hyperparameter and it should be chosen and tuned. You can try different values for hidden layer dimension and observe the results.\n","        - create model\n","    1. Instantiate Loss\n","        - Cross entropy loss\n","        - It also has softmax(logistic function) in it.\n","    1. Instantiate Optimizer\n","        - SGD Optimizer\n","    1. Traning the Model\n","    1. Prediction\n","- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n","- Thanks to hidden layers model learnt better and accuracy(almost 95%) is better than accuracy of logistic regression model."],"metadata":{"_uuid":"ea9eba414f2f0f1e63ef564dc0ee708c753ff51f","_cell_guid":"4d38db05-fad0-468c-9000-20caf5465eca"}},{"cell_type":"code","execution_count":67,"source":["# Import Libraries\r\n","import torch\r\n","import torch.nn as nn\r\n","from torch.autograd import Variable"],"outputs":[],"metadata":{"_uuid":"cf25ee4b28129a47bac4c9dc7d932295155b79f7","_cell_guid":"6925f8ed-54b7-4d9a-9801-acd65f213bc9","execution":{"iopub.status.busy":"2021-08-21T18:08:28.322600Z","iopub.execute_input":"2021-08-21T18:08:28.328592Z","iopub.status.idle":"2021-08-21T18:08:28.333595Z","shell.execute_reply.started":"2021-08-21T18:08:28.328515Z","shell.execute_reply":"2021-08-21T18:08:28.332702Z"},"trusted":true}},{"cell_type":"code","execution_count":68,"source":["# Create ANN Model\r\n","class ANNModel(nn.Module):\r\n","    \r\n","    def __init__(self, input_dim, hidden_dim, output_dim):\r\n","        super(ANNModel, self).__init__()\r\n","        \r\n","        # Linear function 1: 784 --> 150\r\n","        self.fc1 = nn.Linear(input_dim, hidden_dim) \r\n","        # Non-linearity 1\r\n","        self.relu1 = nn.ReLU()\r\n","        \r\n","        # Linear function 2: 150 --> 150\r\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\r\n","        # Non-linearity 2\r\n","        self.tanh2 = nn.Tanh()\r\n","        \r\n","        # Linear function 3: 150 --> 150\r\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\r\n","        # Non-linearity 3\r\n","        self.elu3 = nn.ELU()\r\n","        \r\n","        # Linear function 4 (readout): 150 --> 10\r\n","        self.fc4 = nn.Linear(hidden_dim, output_dim)  \r\n","    \r\n","    def forward(self, x):\r\n","        # Linear function 1\r\n","        out = self.fc1(x)\r\n","        # Non-linearity 1\r\n","        out = self.relu1(out)\r\n","        \r\n","        # Linear function 2\r\n","        out = self.fc2(out)\r\n","        # Non-linearity 2\r\n","        out = self.tanh2(out)\r\n","        \r\n","        # Linear function 2\r\n","        out = self.fc3(out)\r\n","        # Non-linearity 2\r\n","        out = self.elu3(out)\r\n","        \r\n","        # Linear function 4 (readout)\r\n","        out = self.fc4(out)\r\n","        return out\r\n","\r\n","# instantiate ANN\r\n","input_dim = 28*28\r\n","hidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\r\n","output_dim = 10\r\n","\r\n","# Create ANN\r\n","model = ANNModel(input_dim, hidden_dim, output_dim)\r\n","\r\n","# Cross Entropy Loss \r\n","error = nn.CrossEntropyLoss()\r\n","\r\n","# SGD Optimizer\r\n","learning_rate = 0.02\r\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"outputs":[],"metadata":{"_uuid":"cefd0bb2f23b80f30ca65cbb08859ad81ab12e08","_cell_guid":"3472f1c1-5888-4abe-822c-3a493a5f8be5","execution":{"iopub.status.busy":"2021-08-21T18:08:28.335198Z","iopub.execute_input":"2021-08-21T18:08:28.336342Z","iopub.status.idle":"2021-08-21T18:08:28.365277Z","shell.execute_reply.started":"2021-08-21T18:08:28.336273Z","shell.execute_reply":"2021-08-21T18:08:28.364174Z"},"trusted":true}},{"cell_type":"code","execution_count":69,"source":["# ANN model training\r\n","count = 0\r\n","loss_list = []\r\n","iteration_list = []\r\n","accuracy_list = []\r\n","for epoch in range(num_epochs):\r\n","    for i, (images, labels) in enumerate(train_loader):\r\n","\r\n","        train = Variable(images.view(-1, 28*28))\r\n","        labels = Variable(labels)\r\n","        \r\n","        # Clear gradients\r\n","        optimizer.zero_grad()\r\n","        \r\n","        # Forward propagation\r\n","        outputs = model(train)\r\n","        \r\n","        # Calculate softmax and ross entropy loss\r\n","        loss = error(outputs, labels)\r\n","        \r\n","        # Calculating gradients\r\n","        loss.backward()\r\n","        \r\n","        # Update parameters\r\n","        optimizer.step()\r\n","        \r\n","        count += 1\r\n","        \r\n","        if count % 50 == 0:\r\n","            # Calculate Accuracy         \r\n","            correct = 0\r\n","            total = 0\r\n","            # Predict test dataset\r\n","            for images, labels in test_loader:\r\n","\r\n","                test = Variable(images.view(-1, 28*28))\r\n","                \r\n","                # Forward propagation\r\n","                outputs = model(test)\r\n","                \r\n","                # Get predictions from the maximum value\r\n","                predicted = torch.max(outputs.data, 1)[1]\r\n","                \r\n","                # Total number of labels\r\n","                total += len(labels)\r\n","\r\n","                # Total correct predictions\r\n","                correct += (predicted == labels).sum()\r\n","            \r\n","            accuracy = 100 * correct / float(total)\r\n","            \r\n","            # store loss and iteration\r\n","            loss_list.append(loss.data)\r\n","            iteration_list.append(count)\r\n","            accuracy_list.append(accuracy)\r\n","        if count % 500 == 0:\r\n","            # Print Loss\r\n","            print('[*] → Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 500  Loss: 0.8437190055847168  Accuracy: 78.55952453613281 %\n","Iteration: 1000  Loss: 0.4557388722896576  Accuracy: 87.77381134033203 %\n","Iteration: 1500  Loss: 0.2465992122888565  Accuracy: 89.73809814453125 %\n","Iteration: 2000  Loss: 0.30219215154647827  Accuracy: 90.57142639160156 %\n","Iteration: 2500  Loss: 0.3100782632827759  Accuracy: 92.21428680419922 %\n","Iteration: 3000  Loss: 0.11668180674314499  Accuracy: 92.6547622680664 %\n","Iteration: 3500  Loss: 0.2311154305934906  Accuracy: 93.4047622680664 %\n","Iteration: 4000  Loss: 0.06293832510709763  Accuracy: 93.97618865966797 %\n","Iteration: 4500  Loss: 0.3182559907436371  Accuracy: 94.30952453613281 %\n","Iteration: 5000  Loss: 0.10491283237934113  Accuracy: 94.63095092773438 %\n","Iteration: 5500  Loss: 0.21009483933448792  Accuracy: 94.6547622680664 %\n","Iteration: 6000  Loss: 0.18535524606704712  Accuracy: 94.98809814453125 %\n","Iteration: 6500  Loss: 0.0942545160651207  Accuracy: 95.21428680419922 %\n","Iteration: 7000  Loss: 0.11800302565097809  Accuracy: 95.58333587646484 %\n","Iteration: 7500  Loss: 0.10928696393966675  Accuracy: 95.57142639160156 %\n","Iteration: 8000  Loss: 0.19036659598350525  Accuracy: 95.60713958740234 %\n","Iteration: 8500  Loss: 0.06833357363939285  Accuracy: 95.83333587646484 %\n"]}],"metadata":{"_uuid":"c91694f3af94e4e1b76ab01489e186718c70ccd3","_cell_guid":"7550e98b-5011-4d09-88ee-97b0ecbc6f19","execution":{"iopub.status.busy":"2021-08-21T18:08:28.370949Z","iopub.execute_input":"2021-08-21T18:08:28.373654Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# visualization loss \r\n","plt.plot(iteration_list,loss_list)\r\n","plt.xlabel(\"Number of iteration\")\r\n","plt.ylabel(\"Loss\")\r\n","plt.title(\"ANN: Loss vs Number of iteration\")\r\n","plt.show()\r\n","\r\n","# visualization accuracy \r\n","plt.plot(iteration_list,accuracy_list,color = \"red\")\r\n","plt.xlabel(\"Number of iteration\")\r\n","plt.ylabel(\"Accuracy\")\r\n","plt.title(\"ANN: Accuracy vs Number of iteration\")\r\n","plt.show()"],"outputs":[],"metadata":{"_uuid":"c5e2e6da7f1ee801e38358dc28d4c99e32d2b761","_cell_guid":"5579a7d6-7766-4d0f-b9d0-584cb4f28321","trusted":true}}]}